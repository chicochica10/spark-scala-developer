--> Arrancar nc en una consola
ubuntu@ubuntu-VirtualBox:~$ nc -lk 9999


--> Arrancar la consola como spark-shell
ubuntu@ubuntu-VirtualBox:~$ spark-shell --master local[2]


import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext._ // not necessary since Spark 1.3

// Create a local StreamingContext with two working thread and batch interval of 1 second.
// The master requires 2 cores to prevent from a starvation scenario.


// Reutilizamos el spark context de la shell
val ssc = new StreamingContext(sc, Seconds(1))

// Create a DStream that will connect to hostname:port, like localhost:9999
val lines = ssc.socketTextStream("localhost", 9999)

// Split each line into words
val words = lines.flatMap(_.split(" "))

import org.apache.spark.streaming.StreamingContext._ // not necessary since Spark 1.3
// Count each word in each batch
val pairs = words.map(word => (word, 1))
val wordCounts = pairs.reduceByKey(_ + _)

// Print the first ten elements of each RDD generated in this DStream to the console
wordCounts.print()

ssc.start()             // Start the computation
ssc.awaitTermination()  // Wait for the computation to terminate



# Arrancar para escribir en consola
-------------------------------------------
Time: 1452247847000 ms
-------------------------------------------

16/01/08 05:10:47 WARN BlockManager: Block input-0-1452247847400 replicated to only 0 peer(s) instead of 1 peers
-------------------------------------------
Time: 1452247847000 ms
-------------------------------------------
(hola,1)
(a,1)
(todos,1)


-- Fijaros que se escribe cada segundo:
Time: 1452247847000 ms
Time: 1452247847000 ms

-- Cambiar para que el streaming context sea de varios segundos ...
val ssc = new StreamingContext(sc, Seconds(10))

-- Que se fijen en local streaming
Si escribimos varios eventos quedarán registrados en la spark-shell

16/01/08 05:21:11 WARN BlockManager: Block input-0-1452248471600 replicated to only 0 peer(s) instead of 1 peers
16/01/08 05:21:17 WARN BlockManager: Block input-0-1452248476800 replicated to only 0 peer(s) instead of 1 peers
-------------------------------------------
Time: 1452248480000 ms
-------------------------------------------
(adios,1)
(angel,2)
(pepe,1)


Ir ahora a completed batches en http://localhost:4040/streaming/ y ver lo que pasa:
Batch Time	Input Size	Scheduling Delay (?) 	Processing Time (?) 	Total Delay (?) 	Output Ops: Succeeded/Total
2016/01/08 05:21:40 	0 events	2 ms 	36 ms 	38 ms 	1/1
2016/01/08 05:21:30 	0 events	1 ms 	45 ms 	46 ms 	1/1
2016/01/08 05:21:20 	2 events	3 ms 	0.2 s 	0.2 s 	1/1
2016/01/08 05:21:10 	0 events	1 ms 	57 ms 	58 ms 	1/1


// Probar a ejecutar desde código
cd $SPARK_HOME
./bin/run-example streaming.NetworkWordCount localhost 9999


// Collecting tweets:


Consumer Key (API Key) Obtenido desde https://apps.twitter.com/
Consumer Secret (API Secret) Obtenido desde https://apps.twitter.com/ 
Access Token Obtenido desde https://apps.twitter.com/
Access Token Secret Obtenido desde https://apps.twitter.com/ 

https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/TwitterPopularTags.scala
examples/src/main/scala/org/apache/spark/examples/streaming/TwitterPopularTags.scala
ubuntu@ubuntu-VirtualBox:/opt/spark-1.5.2-bin-hadoop2.6/examples/src/main/scala/org/apache/spark/examples/streaming$ 
./bin/run-example streaming.TwitterPopularTags consumerKey consumerSecretKey token secretToken


Con ventana
Let’s illustrate the window operations with an example. Say, you want to extend the earlier example by generating word counts over the last 30 seconds of data, every 10 seconds. To do this, we have to apply the reduceByKey operation on the pairs DStream of (word, 1) pairs over the last 30 seconds of data. This is done using the operation reduceByKeyAndWindow.
val windowedWordCounts = pairs.reduceByKeyAndWindow((a:Int,b:Int) => (a + b), Seconds(30), Seconds(10))
windowedWordCounts.print()

ssc.start()             // Start the computation
ssc.awaitTermination()  // Wait for the computation to terminate


Ver algunos ejemplos en los ejemplos de Spark Streaming
https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala
https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/FlumeEventCount.scala
https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/TwitterHashTagJoinSentiments.scala
https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala
https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala





